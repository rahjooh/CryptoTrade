name: Prod

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: "Type 'deploy' to confirm the production release"
        required: true
        default: deploy
      refresh_dashboard:
        description: "Also update CloudWatch dashboard"
        required: false
        type: boolean
        default: false

concurrency:
  group: ${{ github.workflow }}-${{ github.ref_name }}-production
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  test-ssh:
    name: Verify production SSH connectivity
    if: github.ref == 'refs/heads/main' && github.event.inputs.confirm == 'deploy'
    runs-on: ubuntu-latest
    steps:
      - name: Load ENV secret into $GITHUB_ENV
        shell: bash
        env:
          ENV_SECRET: ${{ secrets.ENV }}   # <-- your multiline secret content
        run: |
          set -Eeuo pipefail
          printf '%s\n' "$ENV_SECRET" \
            | tr -d '\r' \
            | sed -E '/^[[:space:]]*#/d' >> "$GITHUB_ENV"
          
          # sensible defaults (work even with `set -u`)
          : "${EC2_USER:=}"
          : "${DASHBOARD_NAME:=Data}"
          : "${LOG_LEVEL:=warn}"
          if [ -z "${APP_DIR:-}" ] && [ -n "${EC2_USER}" ]; then
            echo "APP_DIR=/home/${EC2_USER}/cryptoflow" >> "$GITHUB_ENV"
          fi
          echo "DASHBOARD_NAME=$DASHBOARD_NAME" >> "$GITHUB_ENV"
          echo "LOG_LEVEL=$LOG_LEVEL" >> "$GITHUB_ENV"
          
          # Decode any *_B64 into a multiline env with heredoc syntax
          # Example: EC2_SSH_KEY_B64 -> EC2_SSH_KEY
          for var in $(compgen -v | grep -E '_B64$' || true); do
          base="${var%_B64}"
          echo "${base}<<EOF" >> "$GITHUB_ENV"
          printf '%s' "${!var}" | base64 -d >> "$GITHUB_ENV"
          echo >> "$GITHUB_ENV"
          echo "EOF" >> "$GITHUB_ENV"
          done      

      - name: Verify loaded env keys
        shell: bash
        run: |
          set -Eeuo pipefail
          echo "Loaded keys (showing only whether set):"
          for k in AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_REGION S3_BUCKET DASHBOARD_NAME LOG_LEVEL EC2_HOST_PROD; do
          if [ "${!k-}" != "" ]; then
           echo "  $k = (set)"
          else
           echo "  $k = (missing/empty)"
          fi
          done

      - name: Check SSH access to production host
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ env.EC2_HOST_PROD }}
          username: ${{ env.EC2_USER }}
          key: ${{ env.EC2_SSH_KEY }}
          script: |
            set -euo pipefail
            echo "Production host reachable as $(whoami)"

  test:
    name: Run test suite
    if: github.ref == 'refs/heads/main' && github.event.inputs.confirm == 'deploy'
    runs-on: ubuntu-latest
    needs: test-ssh
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version-file: go.mod
          cache: false

      - name: Download dependencies
        run: go mod download

      - name: Execute tests
        run: go test ./...

  build:
    name: Build application binary
    if: github.ref == 'refs/heads/main' && github.event.inputs.confirm == 'deploy'
    runs-on: ubuntu-latest
    needs: test
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version-file: go.mod
          cache: false

      - name: Download dependencies
        run: go mod download

      - name: Prune stale binary artifacts (quota guard)
        run: |
          rm -f cryptoflow

      - name: Build binary
        run: go build -o cryptoflow .

      - name: Upload build artifact
        uses: actions/upload-artifact@v4
        with:
          name: production-binary
          path: cryptoflow

      - name: Clean workspace & caches (post)
        if: always()
        run: |
          go clean -cache -modcache
          rm -f cryptoflow

  deploy:
    name: Deploy to production
    if: github.ref == 'refs/heads/main' && github.event.inputs.confirm == 'deploy'
    runs-on: ubuntu-latest
    needs: build
    environment:
      name: production
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ vars.AWS_REGION }}
      S3_BUCKET: ${{ vars.S3_BUCKET }}
      S3_TABLE_ARN: ${{ secrets.S3_TABLE_ARN }}
      LOG_LEVEL: ${{ vars.LOG_LEVEL }}
      DASHBOARD_NAME: Data
    steps:
      - name: Load ENV secret
        env:
          ENV_SECRET: ${{ secrets.ENV }}
        run: |
          set -euo pipefail
          printf '%s\n' "${ENV_SECRET}" | sed -e '/^\s*#/d' -e '/^\s*$/d' >> "$GITHUB_ENV"
          if [ -z "${APP_DIR:-}" ] && [ -n "${EC2_USER:-}" ]; then
            echo "APP_DIR=/home/${EC2_USER}/cryptoflow" >> "$GITHUB_ENV"
          fi
          if [ -z "${DASHBOARD_NAME:-}" ]; then echo "DASHBOARD_NAME=Data" >> "$GITHUB_ENV"; fi
          if [ -z "${LOG_LEVEL:-}" ]; then echo "LOG_LEVEL=warn" >> "$GITHUB_ENV"; fi

      - name: Decode SSH key from base64
        if: env.EC2_SSH_KEY_B64 != ''
        run: |
          set -euo pipefail
          printf 'EC2_SSH_KEY<<EOF\n%s\nEOF\n' "$(printf '%s' "$EC2_SSH_KEY_B64" | base64 -d)" >> "$GITHUB_ENV"
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download build artifact
        uses: actions/download-artifact@v4
        with:
          name: production-binary
          path: artifacts

      - name: Place binary in workspace
        run: mv artifacts/cryptoflow ./cryptoflow

      - name: Ensure remote workspace exists
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ env.EC2_HOST_PROD }}
          username: ${{ env.EC2_USER }}
          key: ${{ env.EC2_SSH_KEY }}
          envs: APP_DIR
          script: |
            set -euo pipefail
            mkdir -p "$APP_DIR"
            cd "$APP_DIR"
            # Preserve .env at APP_DIR root and 'data' directory; clean the rest
            find . -mindepth 1 -maxdepth 1 ! -name '.env' ! -name 'data' -exec rm -rf {} +

      - name: Upload application bundle
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ env.EC2_HOST_PROD }}
          username: ${{ env.EC2_USER }}
          key: ${{ env.EC2_SSH_KEY }}
          source: ".,cryptoflow"
          target: "${{ env.APP_DIR }}"

      - name: Restart services
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ env.EC2_HOST_PROD }}
          username: ${{ env.EC2_USER }}
          key: ${{ env.EC2_SSH_KEY }}
          # Forward app dir and runtime secrets/vars to the host shell
          envs: APP_DIR,AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,AWS_REGION,S3_BUCKET,LOG_LEVEL
          script: |
            set -euo pipefail
            cd "$APP_DIR/infra/podman"

            # Create a .env next to the compose file for Podman Compose
            # Limit perms so it doesn't get world-readable.
            install -m 600 /dev/null .env
            {
              printf 'AWS_ACCESS_KEY_ID=%s\n' "$AWS_ACCESS_KEY_ID"
              printf 'AWS_SECRET_ACCESS_KEY=%s\n' "$AWS_SECRET_ACCESS_KEY"
              printf 'AWS_REGION=%s\n' "${AWS_REGION}"
              printf 'S3_BUCKET=%s\n' "${S3_BUCKET}"
              printf 'LOG_LEVEL=%s\n' "${LOG_LEVEL:-warn}"
            } >> .env

            # Ensure HOME and storage dirs are sane for rootless Podman
            remote_user="$(id -un)"
            remote_home="$(getent passwd "$remote_user" | cut -d: -f6 || true)"
            if [ -z "$remote_home" ]; then remote_home="/home/$remote_user"; fi
            if [ "${HOME:-}" != "$remote_home" ]; then export HOME="$remote_home"; fi
            mkdir -p "$HOME/.local/share/containers/storage"

            # Basic Podman availability check
            if ! command -v podman >/dev/null 2>&1; then
              echo "Podman is not installed on the target host." >&2
              exit 1
            fi

            export APP_ENV=production

            compose_file="podman-compose.yml"
            compose_project="cryptoflow"
            declare -a compose_cmd=()
            if podman compose version >/dev/null 2>&1; then
              compose_cmd=(podman compose -f "$compose_file" -p "$compose_project" --env-file ./.env)
            elif command -v podman-compose >/dev/null 2>&1; then
              compose_cmd=(podman-compose -f "$compose_file" -p "$compose_project" --env-file ./.env)
            else
              echo "Neither 'podman compose' nor 'podman-compose' is available on the target host." >&2
              exit 1
            fi

            # Recreate stack
            "${compose_cmd[@]}" down --remove-orphans || true
            "${compose_cmd[@]}" up -d --build

      - name: Verify Podman container health
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ env.EC2_HOST_PROD }}
          username: ${{ env.EC2_USER }}
          key: ${{ env.EC2_SSH_KEY }}
          envs: APP_DIR
          script: |
            set -euo pipefail
            cd "$APP_DIR/infra/podman"
            container_name="cryptoflow"
            compose_file="podman-compose.yml"
            compose_project="cryptoflow"
            project_label="io.podman.compose.project=$compose_project"
            service_label="io.podman.compose.service=app"

            remote_user="$(id -un)"
            remote_home="$(getent passwd "$remote_user" | cut -d: -f6 || true)"
            if [ -z "$remote_home" ]; then remote_home="/home/$remote_user"; fi
            if [ "${HOME:-}" != "$remote_home" ]; then export HOME="$remote_home"; fi

            declare -a compose_cmd=()
            if podman compose version >/dev/null 2>&1; then
              compose_cmd=(podman compose -f "$compose_file" -p "$compose_project")
            elif command -v podman-compose >/dev/null 2>&1; then
              compose_cmd=(podman-compose -f "$compose_file" -p "$compose_project")
            fi
            if [ ${#compose_cmd[@]} -eq 0 ]; then
              echo "Neither 'podman compose' nor 'podman-compose' is available on the target host." >&2
              exit 1
            fi

            # Prefer health=healthy when a healthcheck is configured; fall back to status=running
            deadline=$((SECONDS + 120))
            while [ "$SECONDS" -lt "$deadline" ]; do
              healthy_container="$(podman ps --filter "name=$container_name" --filter "health=healthy" --format '{{.Names}}' | head -n1 || true)"
              running_container="$(podman ps --filter "name=$container_name" --filter "status=running" --format '{{.Names}}' | head -n1 || true)"

              if [ -n "$healthy_container" ]; then
                echo "Podman container '$healthy_container' is healthy."
                exit 0
              fi
              if [ -z "$healthy_container" ] && [ -n "$running_container" ]; then
                # If no healthcheck exists, treat running as success
                echo "Podman container '$running_container' is running (no healthy container found)."
                exit 0
              fi

              # Try by labels (compose service name)
              healthy_by_label="$(podman ps --filter "label=$project_label" --filter "label=$service_label" --filter "health=healthy" --format '{{.Names}}' | head -n1 || true)"
              running_by_label="$(podman ps --filter "label=$project_label" --filter "label=$service_label" --filter "status=running" --format '{{.Names}}' | head -n1 || true)"
              if [ -n "$healthy_by_label" ]; then
                echo "Podman container '$healthy_by_label' for service 'app' is healthy."
                exit 0
              fi
              if [ -z "$healthy_by_label" ] && [ -n "$running_by_label" ]; then
                echo "Podman container '$running_by_label' for service 'app' is running."
                exit 0
              fi

              echo "Waiting for Podman container '$container_name' to become ready..."
              sleep 5
            done

            echo "Expected Podman container '$container_name' to be ready, but it was not within the timeout." >&2
            project_containers="$(podman ps -a --filter "label=$project_label" --format '{{.Names}}')"
            legacy_container_present=0
            if podman ps -a --filter "name=$container_name" --format '{{.Names}}' | grep -Fxq "$container_name"; then
              legacy_container_present=1
            fi
            if [ -n "$project_containers" ]; then
              echo "Containers for compose project '$compose_project' exist but are not healthy; dumping status and logs..." >&2
              podman ps -a --filter "label=$project_label" || true
              printf '%s\n' "$project_containers" | while IFS= read -r name; do
                [ -n "$name" ] || continue
                echo "----- podman inspect $name -----" >&2
                podman inspect "$name" || true
                echo "----- podman logs $name -----" >&2
                podman logs "$name" || true
              done
            elif [ "$legacy_container_present" -eq 1 ]; then
              echo "Container '$container_name' exists but is not running; dumping status and logs..." >&2
              podman ps -a --filter "name=$container_name" || true
              podman inspect "$container_name" || true
              podman logs "$container_name" || true
            else
              echo "No containers were found for compose project '$compose_project'. Listing all containers." >&2
              podman ps -a || true
            fi
            echo "----- podman compose ps -----" >&2
            "${compose_cmd[@]}" ps || true
            echo "----- podman compose logs -----" >&2
            "${compose_cmd[@]}" logs || true
            exit 1

      - name: Delete binary artifact (post-deploy cleanup)
        if: always()
        run: |
          rm -f cryptoflow
          rm -rf artifacts

      # --- Optional CloudWatch dashboard refresh (commit with [CWdash]) ---
      - name: Configure AWS creds
        if: (github.event_name == 'push' && contains(github.event.head_commit.message, '[CWdash]')) || (github.event_name == 'workflow_dispatch' && github.event.inputs.refresh_dashboard == 'true')
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Resolve prod/stage instances and host info
        if: (github.event_name == 'push' && contains(github.event.head_commit.message, '[CWdash]')) || (github.event_name == 'workflow_dispatch' && github.event.inputs.refresh_dashboard == 'true')
        id: ec2
        env:
          PROD_INSTANCE_ID: ${{ env.PROD_INSTANCE_ID }}
          PROD_INSTANCE_NAME: ${{ env.PROD_INSTANCE_NAME }}
          STAGE_INSTANCE_ID: ${{ env.STAGE_INSTANCE_ID }}
          STAGE_INSTANCE_NAME: ${{ env.STAGE_INSTANCE_NAME }}
        run: |
          set -euo pipefail
          # Helper to resolve an instance ID by ID or Name tag
          resolve_instance() {
            local id="$1" name="$2"
            if [ -n "$id" ] && [ "$id" != "null" ]; then
              echo "$id"
              return 0
            fi
            if [ -n "$name" ] && [ "$name" != "null" ]; then
              aws ec2 describe-instances \
                --filters "Name=tag:Name,Values=$name" "Name=instance-state-name,Values=running" \
                --query "Reservations[].Instances[].InstanceId" --output text
              return 0
            fi
            echo ""; return 0
          }

          PROD_IID=$(resolve_instance "${PROD_INSTANCE_ID:-}" "${PROD_INSTANCE_NAME:-}")
          STAGE_IID=$(resolve_instance "${STAGE_INSTANCE_ID:-}" "${STAGE_INSTANCE_NAME:-}")

          if [ -z "$PROD_IID" ] || [ "$PROD_IID" = "None" ]; then
            echo "Failed to resolve PROD instance (set vars.PROD_INSTANCE_ID or vars.PROD_INSTANCE_NAME)" >&2
            exit 1
          fi
          if [ -z "$STAGE_IID" ] || [ "$STAGE_IID" = "None" ]; then
            echo "Failed to resolve STAGE instance (set vars.STAGE_INSTANCE_ID or vars.STAGE_INSTANCE_NAME)" >&2
            exit 1
          fi

          read -r PROD_PRIVATE_IP PROD_PRIVATE_DNS <<<"$(aws ec2 describe-instances --instance-ids "$PROD_IID" --query 'Reservations[0].Instances[0].[PrivateIpAddress,PrivateDnsName]' --output text)"
          read -r STAGE_PRIVATE_IP STAGE_PRIVATE_DNS <<<"$(aws ec2 describe-instances --instance-ids "$STAGE_IID" --query 'Reservations[0].Instances[0].[PrivateIpAddress,PrivateDnsName]' --output text)"

          # Short hostnames typically look like ip-172-31-1-88
          PROD_HOST_SHORT="${PROD_PRIVATE_DNS%%.*}"
          STAGE_HOST_SHORT="${STAGE_PRIVATE_DNS%%.*}"

          echo "prod_instance_id=$PROD_IID" >> "$GITHUB_OUTPUT"
          echo "stage_instance_id=$STAGE_IID" >> "$GITHUB_OUTPUT"
          echo "prod_ip=$PROD_PRIVATE_IP" >> "$GITHUB_OUTPUT"
          echo "stage_ip=$STAGE_PRIVATE_IP" >> "$GITHUB_OUTPUT"
          echo "prod_host=$PROD_HOST_SHORT" >> "$GITHUB_OUTPUT"
          echo "stage_host=$STAGE_HOST_SHORT" >> "$GITHUB_OUTPUT"

      - name: Build dual-env dashboard body
        if: (github.event_name == 'push' && contains(github.event.head_commit.message, '[CWdash]')) || (github.event_name == 'workflow_dispatch' && github.event.inputs.refresh_dashboard == 'true')
        run: |
          set -euo pipefail
          cp internal/metrics/CWdash.envs.json /tmp/dashboard.json
          sed -i \
            -e "s|<prod-instance-id>|${{ steps.ec2.outputs.prod_instance_id }}|g" \
            -e "s|<stage-instance-id>|${{ steps.ec2.outputs.stage_instance_id }}|g" \
            -e "s|<prod-ip>|${{ steps.ec2.outputs.prod_ip }}|g" \
            -e "s|<stage-ip>|${{ steps.ec2.outputs.stage_ip }}|g" \
            -e "s|<prod-hostname>|${{ steps.ec2.outputs.prod_host }}|g" \
            -e "s|<stage-hostname>|${{ steps.ec2.outputs.stage_host }}|g" \
            /tmp/dashboard.json

      - name: Put dashboard
        if: (github.event_name == 'push' && contains(github.event.head_commit.message, '[CWdash]')) || (github.event_name == 'workflow_dispatch' && github.event.inputs.refresh_dashboard == 'true')
        run: |
          aws cloudwatch put-dashboard \
            --dashboard-name "$DASHBOARD_NAME" \
            --dashboard-body file:///tmp/dashboard.json

      - name: Validate dashboard
        if: (github.event_name == 'push' && contains(github.event.head_commit.message, '[CWdash]')) || (github.event_name == 'workflow_dispatch' && github.event.inputs.refresh_dashboard == 'true')
        run: |
          aws cloudwatch get-dashboard --dashboard-name "$DASHBOARD_NAME" \
            --query 'DashboardValidationMessages' --output table
