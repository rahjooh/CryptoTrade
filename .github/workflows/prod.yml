name: Prod

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: "Type 'deploy' to confirm the production release"
        required: true
        default: deploy

concurrency:
  group: ${{ github.workflow }}-${{ github.ref_name }}-production
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  test-ssh:
    name: Verify production SSH connectivity
    if: github.ref == 'refs/heads/main' && github.event.inputs.confirm == 'deploy'
    runs-on: ubuntu-latest
    steps:
      - name: Check SSH access to production host
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.EC2_HOST_PROD }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            set -euo pipefail
            echo "Production host reachable as $(whoami)"

  test:
    name: Run test suite
    if: github.ref == 'refs/heads/main' && github.event.inputs.confirm == 'deploy'
    runs-on: ubuntu-latest
    needs: test-ssh
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version-file: go.mod
          cache: false

      - name: Download dependencies
        run: go mod download

      - name: Execute tests
        run: go test ./...

  build:
    name: Build application binary
    if: github.ref == 'refs/heads/main' && github.event.inputs.confirm == 'deploy'
    runs-on: ubuntu-latest
    needs: test
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version-file: go.mod
          cache: false

      - name: Download dependencies
        run: go mod download

      - name: Prune stale binary artifacts (quota guard)
        run: |
          rm -f cryptoflow

      - name: Build binary
        run: go build -o cryptoflow .

      - name: Upload build artifact
        uses: actions/upload-artifact@v4
        with:
          name: production-binary
          path: cryptoflow

      - name: Clean workspace & caches (post)
        if: always()
        run: |
          go clean -cache -modcache
          rm -f cryptoflow

  deploy:
    name: Deploy to production
    if: github.ref == 'refs/heads/main' && github.event.inputs.confirm == 'deploy'
    runs-on: ubuntu-latest
    needs: build
    environment:
      name: production
    env:
      APP_DIR: /home/${{ secrets.EC2_USER }}/cryptoflow
      DASHBOARD_NAME: Data

      # Make secrets/vars available in the runner, then forward to SSH.
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
      S3_BUCKET: ${{ secrets.S3_BUCKET }}
      # Prefer org/repo var; default applied on host when writing .env
      LOG_LEVEL: ${{ vars.LOG_LEVEL }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download build artifact
        uses: actions/download-artifact@v4
        with:
          name: production-binary
          path: artifacts

      - name: Place binary in workspace
        run: mv artifacts/cryptoflow ./cryptoflow

      - name: Ensure remote workspace exists
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.EC2_HOST_PROD }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}
          envs: APP_DIR
          script: |
            set -euo pipefail
            mkdir -p "$APP_DIR"
            cd "$APP_DIR"
            # Preserve .env at APP_DIR root and 'data' directory; clean the rest
            find . -mindepth 1 -maxdepth 1 ! -name '.env' ! -name 'data' -exec rm -rf {} +

      - name: Upload application bundle
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.EC2_HOST_PROD }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}
          source: ".,cryptoflow"
          target: "${{ env.APP_DIR }}"

      - name: Restart services
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.EC2_HOST_PROD }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}
          # Forward app dir and runtime secrets/vars to the host shell
          envs: APP_DIR,AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,AWS_REGION,S3_BUCKET,LOG_LEVEL
          script: |
            set -euo pipefail
            cd "$APP_DIR/infra/podman"

            # Create a .env next to the compose file for Podman Compose
            # Limit perms so it doesn't get world-readable.
            install -m 600 /dev/null .env
            {
              printf 'AWS_ACCESS_KEY_ID=%s\n' "$AWS_ACCESS_KEY_ID"
              printf 'AWS_SECRET_ACCESS_KEY=%s\n' "$AWS_SECRET_ACCESS_KEY"
              printf 'AWS_REGION=%s\n' "${AWS_REGION}"
              printf 'S3_BUCKET=%s\n' "${S3_BUCKET}"
              printf 'LOG_LEVEL=%s\n' "${LOG_LEVEL:-warn}"
            } >> .env

            # Ensure HOME and storage dirs are sane for rootless Podman
            remote_user="$(id -un)"
            remote_home="$(getent passwd "$remote_user" | cut -d: -f6 || true)"
            if [ -z "$remote_home" ]; then remote_home="/home/$remote_user"; fi
            if [ "${HOME:-}" != "$remote_home" ]; then export HOME="$remote_home"; fi
            mkdir -p "$HOME/.local/share/containers/storage"

            # Basic Podman availability check
            if ! command -v podman >/dev/null 2>&1; then
              echo "Podman is not installed on the target host." >&2
              exit 1
            fi

            export APP_ENV=production

            compose_file="podman-compose.yml"
            compose_project="cryptoflow"
            declare -a compose_cmd=()
            if podman compose version >/dev/null 2>&1; then
              compose_cmd=(podman compose -f "$compose_file" -p "$compose_project" --env-file ./.env)
            elif command -v podman-compose >/dev/null 2>&1; then
              compose_cmd=(podman-compose -f "$compose_file" -p "$compose_project" --env-file ./.env)
            else
              echo "Neither 'podman compose' nor 'podman-compose' is available on the target host." >&2
              exit 1
            fi

            # Recreate stack
            "${compose_cmd[@]}" down --remove-orphans || true
            "${compose_cmd[@]}" up -d --build

      - name: Verify Podman container health
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.EC2_HOST_PROD }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            set -euo pipefail
            container_name="cryptoflow"
            compose_file="podman-compose.yml"
            compose_project="cryptoflow"
            project_label="io.podman.compose.project=$compose_project"
            service_label="io.podman.compose.service=app"

            remote_user="$(id -un)"
            remote_home="$(getent passwd "$remote_user" | cut -d: -f6 || true)"
            if [ -z "$remote_home" ]; then remote_home="/home/$remote_user"; fi
            if [ "${HOME:-}" != "$remote_home" ]; then export HOME="$remote_home"; fi

            declare -a compose_cmd=()
            if podman compose version >/dev/null 2>&1; then
              compose_cmd=(podman compose -f "$compose_file" -p "$compose_project")
            elif command -v podman-compose >/dev/null 2>&1; then
              compose_cmd=(podman-compose -f "$compose_file" -p "$compose_project")
            fi
            if [ ${#compose_cmd[@]} -eq 0 ]; then
              echo "Neither 'podman compose' nor 'podman-compose' is available on the target host." >&2
              exit 1
            fi

            # Prefer health=healthy when a healthcheck is configured; fall back to status=running
            deadline=$((SECONDS + 120))
            while [ "$SECONDS" -lt "$deadline" ]; do
              healthy_container="$(podman ps --filter "name=$container_name" --filter "health=healthy" --format '{{.Names}}' | head -n1 || true)"
              running_container="$(podman ps --filter "name=$container_name" --filter "status=running" --format '{{.Names}}' | head -n1 || true)"

              if [ -n "$healthy_container" ]; then
                echo "Podman container '$healthy_container' is healthy."
                exit 0
              fi
              if [ -z "$healthy_container" ] && [ -n "$running_container" ]; then
                # If no healthcheck exists, treat running as success
                echo "Podman container '$running_container' is running (no healthy container found)."
                exit 0
              fi

              # Try by labels (compose service name)
              healthy_by_label="$(podman ps --filter "label=$project_label" --filter "label=$service_label" --filter "health=healthy" --format '{{.Names}}' | head -n1 || true)"
              running_by_label="$(podman ps --filter "label=$project_label" --filter "label=$service_label" --filter "status=running" --format '{{.Names}}' | head -n1 || true)"
              if [ -n "$healthy_by_label" ]; then
                echo "Podman container '$healthy_by_label' for service 'app' is healthy."
                exit 0
              fi
              if [ -z "$healthy_by_label" ] && [ -n "$running_by_label" ]; then
                echo "Podman container '$running_by_label' for service 'app' is running."
                exit 0
              fi

              echo "Waiting for Podman container '$container_name' to become ready..."
              sleep 5
            done

            echo "Expected Podman container '$container_name' to be ready, but it was not within the timeout." >&2
            project_containers="$(podman ps -a --filter "label=$project_label" --format '{{.Names}}')"
            legacy_container_present=0
            if podman ps -a --filter "name=$container_name" --format '{{.Names}}' | grep -Fxq "$container_name"; then
              legacy_container_present=1
            fi
            if [ -n "$project_containers" ]; then
              echo "Containers for compose project '$compose_project' exist but are not healthy; dumping status and logs..." >&2
              podman ps -a --filter "label=$project_label" || true
              printf '%s\n' "$project_containers" | while IFS= read -r name; do
                [ -n "$name" ] || continue
                echo "----- podman inspect $name -----" >&2
                podman inspect "$name" || true
                echo "----- podman logs $name -----" >&2
                podman logs "$name" || true
              done
            elif [ "$legacy_container_present" -eq 1 ]; then
              echo "Container '$container_name' exists but is not running; dumping status and logs..." >&2
              podman ps -a --filter "name=$container_name" || true
              podman inspect "$container_name" || true
              podman logs "$container_name" || true
            else
              echo "No containers were found for compose project '$compose_project'. Listing all containers." >&2
              podman ps -a || true
            fi
            echo "----- podman compose ps -----" >&2
            "${compose_cmd[@]}" ps || true
            echo "----- podman compose logs -----" >&2
            "${compose_cmd[@]}" logs --no-color || true
            exit 1

      - name: Delete binary artifact (post-deploy cleanup)
        if: always()
        run: |
          rm -f cryptoflow
          rm -rf artifacts

      # --- Optional CloudWatch dashboard refresh (commit with [CWdash]) ---
      - name: Configure AWS creds
        if: contains(github.event.head_commit.message, '[CWdash]')
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION }}

      - name: Resolve collector InstanceId
        if: contains(github.event.head_commit.message, '[CWdash]')
        id: ec2
        run: |
          IID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=Collector Test 2" "Name=instance-state-name,Values=running" \
            --query "Reservations[].Instances[].InstanceId" --output text)
          if [ -z "$IID" ]; then echo "No running collector instance found"; exit 1; fi
          echo "instance_id=$IID" >> "$GITHUB_OUTPUT"

      - name: Inject InstanceId into dashboard JSON
        if: contains(github.event.head_commit.message, '[CWdash]')
        run: |
          cp internal/metrics/CWdash.json /tmp/dashboard.json
          sed -i "s|<collector-instance-id>|${{ steps.ec2.outputs.instance_id }}|g" /tmp/dashboard.json

      - name: Put dashboard
        if: contains(github.event.head_commit.message, '[CWdash]')
        run: |
          aws cloudwatch put-dashboard \
            --dashboard-name "$DASHBOARD_NAME" \
            --dashboard-body file:///tmp/dashboard.json

      - name: Validate dashboard
        if: contains(github.event.head_commit.message, '[CWdash]')
        run: |
          aws cloudwatch get-dashboard --dashboard-name "$DASHBOARD_NAME" \
            --query 'DashboardValidationMessages' --output table
