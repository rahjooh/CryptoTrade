# ðŸ£ CryptoFlow â€“ High-Frequency Binance Orderbook Collector

**CryptoFlow** is a Go-based high-frequency data collector that captures the BTCUSDT order book snapshot from Binance every second. It splits the snapshot into `bids` and `asks`, stores them in hourly rotated Parquet files, and exposes Prometheus metrics for observability.

---

## ðŸ“¦ Features

- ðŸ§  **Async, functional, and isolated architecture**
- ðŸ“ **Hourly Parquet file storage** for `bids` and `asks`
- ðŸ” **Writer pooling** for performance and memory safety
- ðŸ“ˆ **Prometheus metrics** exposed on port `2112`
- ðŸ“¦ **Dockerized and Makefile-driven** for easy build/run
- âœ… **Graceful shutdown** and clean logging with `zerolog`

---

## ðŸ“‚ Directory Structure

```
CryptoFlow/
â”œâ”€â”€ cmd/CryptoFlow/main.go                 # App entrypoint
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ config/config.go              # File paths, output dirs, HTTP client
â”‚   â”œâ”€â”€ logger/logger.go              # Structured logging with zerolog
â”‚   â”œâ”€â”€ metrics/metrics.go            # Prometheus metric registration
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ snapshot.go               # Binance response models
â”‚   â”‚   â””â”€â”€ orderbook_row.go         # Parquet row schema
â”‚   â”œâ”€â”€ reader/fetcher.go            # HTTP fetch logic
â”‚   â””â”€â”€ writer/pool.go               # Writer pool, hourly parquet rotation
â”œâ”€â”€ Dockerfile                        # Build image
â”œâ”€â”€ docker-compose.yml               # Optional for orchestration
â”œâ”€â”€ Makefile                          # CLI automation
â”œâ”€â”€ go.mod / go.sum                   # Dependencies
â””â”€â”€ README.md                         # â† This file
```

---

## âš™ï¸ How It Works

1. Every second:
   - Fetch order book snapshot from Binance
   - Split into `bids` and `asks`
2. Each entry is converted into a table row:
   - `timestamp`, `price`, `quantity`
3. Rows are written into Parquet files:
   - Stored hourly under `./data/bids/YYYY-MM-DD_HH.parquet`
   - And `./data/asks/YYYY-MM-DD_HH.parquet`
4. Prometheus metrics are updated
5. Logs are written using `zerolog`

---

## ðŸš€ Getting Started

### ðŸ§° Requirements

- Docker
- `make`

### ðŸ”¨ Build the Docker image

```bash
make docker-build
```

### â–¶ï¸ Run the container

```bash
make docker-run
```

This will:

- Run the collector
- Write output to `./data` on your host
- Expose metrics at [http://localhost:2112/metrics](http://localhost:2112/metrics)

---

## ðŸ“Š Metrics Exposed

| Metric                             | Description                                 |
|------------------------------------|---------------------------------------------|
| `CryptoFlow_snapshot_success_total`     | Number of successful orderbook snapshots    |
| `CryptoFlow_snapshot_errors_total`      | Number of failed fetch/write attempts       |
| `go_goroutines` / `go_memstats_*` | Runtime stats (GC, heap, threads, etc.)     |
| `process_*`                        | CPU, RAM, file descriptor stats             |

---

## ðŸ“ Output File Structure

Files are stored under:

```
./data/
â”œâ”€â”€ bids/
â”‚   â”œâ”€â”€ 2025-07-29_08.parquet
â”‚   â”œâ”€â”€ 2025-07-29_09.parquet
â”‚   â””â”€â”€ ...
â””â”€â”€ asks/
    â”œâ”€â”€ 2025-07-29_08.parquet
    â”œâ”€â”€ 2025-07-29_09.parquet
    â””â”€â”€ ...
```

Each file includes one hour of 1-second resolution data for the respective side.

---

## ðŸ“˜ Code Highlights

- **Writer Pool**: Prevents memory leaks by managing file handles per hour.
- **Parquet Format**: Fast, columnar storage ideal for analytics.
- **Zerolog**: Low overhead structured logging.
- **Prometheus**: Native Go instrumentation and `/metrics` endpoint.
- **Graceful Shutdown**: Catches SIGINT and SIGTERM to flush & close writers.

---

## ðŸ›  Makefile Commands

```bash
make build          # Build CryptoFlow binary
make run            # Run locally (without Docker)
make docker-build   # Build Docker image
make docker-run     # Run Docker container (with volume + metrics port)
make clean          # Remove compiled binary
```

---

## ðŸ” Notes on Production

- Consider rate limits on Binance API
- Add exponential backoff for retries
- Rotate or offload `.parquet` files to cloud/S3
- Add `/healthz` or `/readyz` endpoints
- Use Prometheus + Grafana to visualize metrics

---

## ðŸ§ª Future Enhancements

- Snapshot duration histogram
- Retry & backoff logic
- S3 uploader for parquet files
- Unit tests with mock HTTP client
- Alerting on error spike or data gaps

---

## ðŸ§  Credits

Developed with â¤ï¸ using Go, Zerolog, Prometheus, and Parquet.

---

> _Built for speed, clarity, and observability._